Prediction assignment
========================================================

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```


```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
setwd("C:/Users/Robin/Dropbox/Rcourse/pml")
```

```{r, results='hide', message=FALSE, warning=FALSE}
library(caret)
library(randomForest)
set.seed(1234)
```


The training data (downloaded from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) is loaded, and then the first 6 variables are removed. These variables are labels rather than predictors, so they are not useful for a prediction algorithm. Additionally the predictors that have more than half the values missing (na) are removed.

Finally, the data is split into 80% training and 20% testing sets.


```{r, results='hide', message=FALSE, warning=FALSE}

training<-read.csv('pml-training.csv')
#remove username etc.
train2<-training[,-c(1:6)]
#remove variables with more than hlf na
highNA<- sapply(train2, function(x) sum(is.na(x)))/dim(train2)[1]
train3<-train2[,highNA<.5]


inMyTrain<-createDataPartition(y=train3$classe,p=.8,list=F)
myTrain<-train3[inMyTrain,]
myTest<-train3[-inMyTrain,]

```

The preprocessing steps we do on the data are, firstly removing the predictors with near zero variance, using the nearZeroVar function, and secondly doing principal component analysis (PCA) to find out how many components are required for 80%, 90%, and 95% of the variance. The PCA in this chunk is diagnostic only, as the train method in caret has PCA options included.

```{r, message=FALSE, warning=FALSE}
#remove  classe
classeindex<-match('classe',names(myTrain))
myTrain2<-myTrain[,-c(classeindex)]
nzv<-nearZeroVar(myTrain2)
myTrain3<-myTrain2[,-c(nzv)]

#same on my test
myTest2<-myTest[,-c(classeindex)]
myTest3<-myTest2[,-c(nzv)]
myTest3$classe<-myTest$classe

PCA80 <- preProcess(myTrain3,method="pca",thresh=.8)
PCA90 <- preProcess(myTrain3,method="pca",thresh=.9)
PCA95 <- preProcess(myTrain3,method="pca",thresh=.95) 
PCA80 #13 components
PCA90 #19 components
PCA95 #25 components

myTrain3$classe<-myTrain$classe
```

We train our model using the random forest method in the caret library. This method has the advantage that it has in-built cross validation.

The preprocessing used on this data is principal component analysis encompassing 80% of the variance. This is chosen to reduce the load on the computer, as this training is quite intensive.

The model is then used to predict the outcome on the training and test sets.


```{r, message=FALSE, warning=FALSE}

modFit<-train(classe~.,myTrain3,method='rf', preProcess='pca', trControl = trainControl(preProcOptions = list(thresh = 0.8 ) ) )

testPred<- predict(modFit,myTest3)
trainPred<- predict(modFit,myTrain3)
confusionMatrix(trainPred, myTrain3$classe)
confusionMatrix(testPred, myTest3$classe)
```

As we can see, the model has 100% accuracy on the training set, which makes us wary of overfitting, though it still achieves 97% accuracy on the test set, so it is not a bad model


Now we get the predictions from the final test set

```{r, message=FALSE, warning=FALSE}

testing<-read.csv('pml-testing.csv')
#remove username etc.
test2<-testing[,-c(1:6)]

test3<-test2[,highNA<.5]

test4<-test3[,-c(nzv)]


finalTest<- predict(modFit,test4)

finalTest


```